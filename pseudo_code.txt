######################################################################################
        Main loop
######################################################################################

target_spectrum = raman_system.apply_input(input_spectrum, power, wavelength)

control_loop = make_control_loop(raman_system, controller)

control_loop.set_target(target_spectrum)

for i in step_number:
    control_loop.step()


######################################################################################
        Control loop step
######################################################################################

current_output = raman_system.get_output()
current_control = controller.get_control(current_input, current_output, target_output)
controller.update(current_input, current_output, target_output)
controller.update_controller(error)
raman_system.apply_control(current_control)


######################################################################################
        Bernoulli get control
######################################################################################

probs = sigmoid(logits)

distribution = Bernoulli(probs)

while length(samples) < samples_number
    sample = get_sample(distribution)
    samples.add(sample)
sample = mean(samples)

action = convert_to_action(sample)

return action


######################################################################################
        Bernoulli update
######################################################################################

reward = calculate_reward(current_output, target_output)

baseline = gamma * baseline + (1 - gamma) * reward

advantage = beta * (reward - baseline)

eligibility = sample - probs

update = learning_rate * advantage * eligibility - weight_decay * logits

logits += update


######################################################################################
        Reward function
######################################################################################

shape_loss = |normalized_output_spectrum - normalized_target_spectrum|

integral_loss = integral(currrent_output_spectrum) - integral(target_spectrum)

loss = shape_loss + integral_loss

reward = - loss